{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32b5413f",
   "metadata": {},
   "source": [
    "### Question 1: What is Simple Linear Regression?\n",
    "Simple Linear Regression is a statistical method used to model the relationship between a dependent variable (target) and a single independent variable (predictor). It assumes a linear relationship between the variables, represented by the equation: \n",
    "\n",
    "**Y = β₀ + β₁X + ε**, where:\n",
    "- Y is the dependent variable,\n",
    "- X is the independent variable,\n",
    "- β₀ is the y-intercept,\n",
    "- β₁ is the slope of the line,\n",
    "- ε is the error term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2754fd1b",
   "metadata": {},
   "source": [
    "### Question 2: What are the key assumptions of Simple Linear Regression?\n",
    "1. Linearity: The relationship between X and Y is linear.\n",
    "2. Independence: Observations are independent of each other.\n",
    "3. Homoscedasticity: Constant variance of errors.\n",
    "4. Normality: Residuals (errors) are normally distributed.\n",
    "5. No multicollinearity (though more relevant to multiple regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa34f24d",
   "metadata": {},
   "source": [
    "### Question 3: What is heteroscedasticity, and why is it important to address in regression models?\n",
    "Heteroscedasticity occurs when the variance of the residuals is not constant across all levels of the independent variable. \n",
    "It violates the assumption of homoscedasticity, leading to inefficient estimates and unreliable statistical tests (e.g., t-tests and F-tests). \n",
    "Addressing it ensures more reliable regression results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244fe310",
   "metadata": {},
   "source": [
    "### Question 4: What is Multiple Linear Regression?\n",
    "Multiple Linear Regression is a statistical technique used to model the relationship between one dependent variable and two or more independent variables. \n",
    "It extends simple linear regression by incorporating multiple predictors. The model is:\n",
    "**Y = β₀ + β₁X₁ + β₂X₂ + ... + βₙXₙ + ε**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6160c27",
   "metadata": {},
   "source": [
    "### Question 5: What is polynomial regression, and how does it differ from linear regression?\n",
    "Polynomial regression is a type of regression where the relationship between the independent variable and the dependent variable is modeled as an nth-degree polynomial.\n",
    "It captures non-linear relationships, unlike simple linear regression which assumes a straight-line relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d000dc",
   "metadata": {},
   "source": [
    "### Question 6: Simple Linear Regression Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bddb3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Sample data\n",
    "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
    "Y = np.array([2.1, 4.3, 6.1, 7.9, 10.2])\n",
    "\n",
    "# Model fitting\n",
    "model = LinearRegression()\n",
    "model.fit(X, Y)\n",
    "Y_pred = model.predict(X)\n",
    "\n",
    "# Plot\n",
    "plt.scatter(X, Y, color='blue', label='Data')\n",
    "plt.plot(X, Y_pred, color='red', label='Regression Line')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.title('Simple Linear Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a30588",
   "metadata": {},
   "source": [
    "### Question 7: Multiple Linear Regression and VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e025a252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Data\n",
    "data = pd.DataFrame({\n",
    "    'Area': [1200, 1500, 1800, 2000],\n",
    "    'Rooms': [2, 3, 3, 4],\n",
    "    'Price': [250000, 300000, 320000, 370000]\n",
    "})\n",
    "\n",
    "# Features and target\n",
    "X = data[['Area', 'Rooms']]\n",
    "y = data['Price']\n",
    "\n",
    "# VIF Calculation\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['feature'] = X.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "# Model fitting\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "vif_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555b6ca2",
   "metadata": {},
   "source": [
    "### Question 8: Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45657db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Data\n",
    "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
    "Y = np.array([2.2, 4.8, 7.5, 11.2, 14.7])\n",
    "\n",
    "# Polynomial transformation\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Model\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, Y)\n",
    "Y_pred = model.predict(X_poly)\n",
    "\n",
    "# Plot\n",
    "plt.scatter(X, Y, color='blue', label='Data')\n",
    "plt.plot(X, Y_pred, color='green', label='Polynomial Fit')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.title('Polynomial Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be5b89d",
   "metadata": {},
   "source": [
    "### Question 9: Residuals Plot and Heteroscedasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab08353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "X = np.array([10, 20, 30, 40, 50]).reshape(-1, 1)\n",
    "Y = np.array([15, 35, 40, 50, 65])\n",
    "\n",
    "# Model\n",
    "model = LinearRegression()\n",
    "model.fit(X, Y)\n",
    "Y_pred = model.predict(X)\n",
    "residuals = Y - Y_pred\n",
    "\n",
    "# Plot\n",
    "plt.scatter(X, residuals)\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.title('Residuals Plot')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0730d726",
   "metadata": {},
   "source": [
    "### Question 10: Addressing Heteroscedasticity and Multicollinearity\n",
    "To address **heteroscedasticity**:\n",
    "- Use transformations (e.g., log, sqrt) on the dependent variable.\n",
    "- Apply Weighted Least Squares (WLS).\n",
    "- Use robust standard errors.\n",
    "\n",
    "To address **multicollinearity**:\n",
    "- Remove or combine correlated predictors.\n",
    "- Use Principal Component Analysis (PCA).\n",
    "- Regularization techniques like Ridge or Lasso Regression."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
